
## CAP 理论
### 含义

含义 CAP 是指一个分布式系统最多只能满足一致性 Consistency ，可用性 Availability 和 分区容忍性 Partition Tolerance 这三项中的两项。  

> **一致性**          

是指在所有的节点看的数据都是一样的 ，相同的，也就是更新操作成功之后返回给客户端完成后，所有节点在同一时间的数据完全一致，等同于所有节点拥有数据的最新版本。 

> **可用性**   

是指在任何时候，都写都是成功的，也就是服务一直可用，而且是正常响应时间。 比如系统的稳定是4个9 ，99.99% ，也就是SLA【服务水平协议】。  

> **分区容忍性**    

 是指当部分节点出现消息丢失或者分区故障的时候，分布式系统仍然能够继续运行。即系统容忍网络出现分区，并且遇到某节点或者网络分区之间不可达的情况下，仍然能够对外提供满足
 一致性和可用性的服务。

> **证明三者不能同时成立**

clientA- writex=1 success --> server1  ----read x=1 -> client 1

clientA-writex=1 falied      -->     server2  ----read x=0 -> client 2

因为允许分区容错，Write 操作可能在 Server 1 上成功，在 Server 2 上失败，这时候对于 Client 1 和 Client 2，就会读取到不一致的值，出现不一致的情况。如果要保持 X 值的一致性，Write 操作必须同时失败， 也就是降低系统的可用性。

在该证明中，对 CAP 的定义进行了更明确的声明：

Consistency，一致性被称为原子对象，任何的读写都应该看起来是“原子”的，或串行的，写后面的读一定能读到前面写的内容，所有的读写请求都好像被全局排序；

Availability，对任何非失败节点都应该在有限时间内给出请求的回应（请求的可终止性）；

Partition Tolerance，允许节点之间丢失任意多的消息，当网络分区发生时，节点之间的消息可能会完全丢失。



在分布式系统中，由于系统的各层拆分，P是确定的，CAP的应用模型就是CP架构和AP架构，分布式系统所关注的，就是在P 的情况下，如何实现更好的A或者更稳的C

> **CP 和 AP 架构的取舍**

在通常的分布式系统中，为了保证数据的高可用，通常会将数据保留多个副本（Replica），网络分区是既成的现实，于是只能在可用性和一致性两者间做出选择。CAP 理论关注的是在绝对情况下，在工程上，可用性和一致性并不是完全对立的，我们关注的往往是如何在保持相对一致性的前提下，提高系统的可用性。



业务上对一致性的要求会直接反映在系统设计中，典型的就是 CP 和 AP 结构。

### CP 架构：

对于 CP 来说，放弃可用性，追求一致性和分区容错性。

 ZooKeeper，就是采用了 CP 一致性，ZooKeeper 是一个分布式的服务框架，主要用来解决分布式集群中应用系统的协调和一致性问题。其核心算法是 Zab，所有设计都是为了一致性。在 CAP 模型中，ZooKeeper 是 CP，这意味着面对网络分区时，为了保持一致性，它是不可用的。

### AP 架构

对于 AP 来说，放弃强一致性，追求分区容错性和可用性，这是很多分布式系统设计时的选择，后面的 Base 也是根据 AP 来扩展的。

Eureka 的各个节点都是平等的，几个节点挂掉不影响正常节点的工作，剩余的节点依然可以提供注册和查询服务，只要有一台 Eureka 还在，就能保证注册服务可用，只不过查到的信息可能不是最新的版本，不保证一致性。

### BASE理论

#### 含义

Base 是三个短语的简写，即基本可用（Basically Available）、软状态（Soft State）和最终一致性（Eventually Consistent）。

Base 理论的核心思想是最终一致性，即使无法做到强一致性（Strong Consistency），但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性（Eventual Consistency）

#### 基本可用

基本可用比较好理解，就是不追求 CAP 中的「任何时候，读写都是成功的」，而是系统能够基本运行，一直提供服务。基本可用强调了分布式系统在出现不可预知故障的时候，允许损失部分可用性，相比正常的系统，可能是响应时间延长，或者是服务被降级。

比如流量很大的时候，双十一，如果超过了系统所承受的负载或者qps峰值，可以进行相应的限流和降级措施来保证服务的基本可用，而不是因为流量过大，导致整个服务瘫痪，所有流量都丢失。

#### 软状态

是指允许系统中的数据存在中间状态，并认为该状态不影响系统的整体可用性，也就是说允许系统在多个不同的节点数据副本存在数据延迟。

#### 最终一致性

也就是说数据不可能是一直在软状态，必须在一个时间期限之后达到各个节点数据的一致性，在期限过后，应该达到各个节点的副本数据的一致性，也就是最终一致了。

到达最终一致性的时间 ，就是不一致窗口时间，在没有故障发生的前提下，不一致窗口的时间主要受通信延迟，系统负载和复制副本的个数影响。



#### 不同数据一致性模型

一般来说，数据一致性模型可以分为强一致性和弱一致性，强一致性也叫做线性一致性，除此以外，所有其他的一致性都是弱一致性的特殊情况。弱一致性根据不同的业务场景，又可以分解为更细分的模型，不同一致性模型又有不同的应用场景。

在互联网领域的绝大多数场景中，都需要牺牲强一致性来换取系统的高可用性，系统往往只需要保证“最终一致性”，只要这个最终时间是在用户可以接受的范围内即可。

<img src="imgs/Ciqah16FruiAGz3eAAIrOBxKnpU229.png" alt="img" style="zoom:50%;" />





##### 强一致性

当更新操作完成之后，任何多个后续进程的访问都会返回最新的更新过的值，这种是对用户最友好的，就是用户上一次写什么，下一次就保证能读到什么。根据 CAP 理论，这种实现需要牺牲可用性。

##### 弱一致性

系统在数据写入成功之后，不承诺立即可以读到最新写入的值，也不会具体的承诺多久之后可以读到。用户读到某一操作对系统数据的更新需要一段时间，我们称这段时间为“不一致性窗口”。

##### 最终一致性

最终一致性是弱一致性的特例，强调的是所有的数据副本，在经过一段时间的同步之后，最终都能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。

到达最终一致性的时间 ，就是不一致窗口时间，在没有故障发生的前提下，不一致窗口的时间主要受通信延迟，系统负载和复制副本的个数影响。



### CAP 及 Base 的关系

Base 理论是在 CAP 上发展的，CAP 理论描述了分布式系统中数据一致性、可用性、分区容错性之间的制约关系，当你选择了其中的两个时，就不得不对剩下的一个做一定程度的牺牲。

Base 理论则是对 CAP 理论的实际应用，也就是在分区和副本存在的前提下，通过一定的系统设计方案，放弃强一致性，实现基本可用，这是大部分分布式系统的选择，比如 NoSQL 系统、微服务架构

除了 CAP 和 Base，上面还提到了 ACID 原理，ACID 是一种强一致性模型，强调原子性、一致性、隔离性和持久性，主要用于在数据库实现中。Base 理论面向的是高可用、可扩展的分布式系统，ACID 适合传统金融等业务，在实际场景中，不同业务对数据的一致性要求不一样，ACID 和 Base 理论往往会结合使用。

## 分布式算法

### WARO协议 [Write All Read one]

是一种简单的副本控制协议，当 Client 请求向某副本写数据时（更新数据），只有当所有的副本都更新成功之后，这次写操作才算成功，否则视为失败。这样的话，只需要读任何一个副本上的数据即可。但是WARO带来的影响是写服务的可用性较低，因为只要有一个副本更新失败，此次写操作就视为失败了。

### Quorum机制

Quorum 的定义如下：假设有 N 个副本，更新操作 wi 在 W 个副本中更新成功之后，则认为此次更新操作 wi 成功，把这次成功提交的更新操作对应的数据叫做：“成功提交的数据”。对于读操作而言，至少需要读 R 个副本，其中，W+R>N ，即 W 和 R 有重叠，一般，W+R=N+1。

- N = 存储数据副本的数量
- W = 更新成功所需的副本
- R = 一次数据对象读取要访问的副本的数量

听起来有些抽象，举个例子：

假设我有5个副本，更新操作成功写入了3个，另外2个副本仍是旧数据，此时在读取的时候，只要确保读取副本的数量大于2，那么肯定就会读到最新的数据。至于如何确定哪份数据是最新的，我们可以通过引入数据版本号的方式判断（Quorum 机制的使用需要配合一个获取最新成功提交的版本号的 metadata 服务，这样可以确定最新已经成功提交的版本号，然后从已经读到的数据中就可以确认最新写入的数据。）

### Paxos

> 介绍

https://www.bilibili.com/video/BV1TW411M7Fx?from=search&seid=6856473444955101675

### Raft

> 介绍

https://zhuanlan.zhihu.com/p/91288179

> 动画演示

http://thesecretlivesofdata.com/raft/

> 测试

https://raft.github.io/



nacos 就是raft 实现



### Zab

> 介绍

https://www.cnblogs.com/stateis0/p/9062133.html

https://blog.csdn.net/liuchang19950703/article/details/111406622

https://www.cnblogs.com/dyg0826/p/11115117.html

> 选举过程

选举过程如下：



1.各个节点变更状态，变更为 Looking

ZooKeeper 中除了 Leader 和 Follower，还有 Observer 节点，Observer 不参与选举，Leader 挂后，余下的 Follower 节点都会将自己的状态变更为 Looking，然后开始进入 Leader 选举过程。



2.各个 Server 节点都会发出一个投票，参与选举

在第一次投票中，所有的 Server 都会投自己，然后各自将投票发送给集群中所有机器，在运行期间，每个服务器上的 Zxid 大概率不同。



3.集群接收来自各个服务器的投票，开始处理投票和选举

处理投票的过程就是对比 Zxid 的过程，假定 Server3 的 Zxid 最大，Server1 判断 Server3 可以成为 Leader，那么 Server1 就投票给 Server3，判断的依据如下：

首先选举 epoch 最大的

如果 epoch 相等，则选 zxid 最大的

若 epoch 和 zxid 都相等，则选择 server id 最大的，就是配置 zoo.cfg 中的 myid

在选举过程中，如果有节点获得超过半数的投票数，则会成为 Leader 节点，反之则重新投票选举。

### 区块链

